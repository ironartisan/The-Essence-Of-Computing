

# 第1章 毫厘千里之差-大O概念

## 1.1　算法的规范化和量化度量

冯·诺依曼发明计算机体系结构和高德纳编写TeX程序似乎都是偶然为之的结果。冯·诺依曼原本不想发明计算机，他只想算题；高德纳也不想发明排版软件，他只想写书。为什么这些大师们偶然为之的工作比二流人才穷其一生的发现有时还有影响力呢？因为除了能力的差异外，他们还有着遇到问题时解决问题的积极态度。任何人在前进的过程中都会遇到问题，但是对待问题的态度决定了个人的命运。



## 1.2 大数和数量级的概念

场景1：使用1万个数据进行测试，算法A运行1毫秒，算法B则需要运行10毫秒。

场景2：使用100万个数据进行测试，算法A运行10000毫秒，算法B运行6000毫秒。

那么到底哪个算法更好一些呢？如果单纯从场景1做判断，显然是算法A好，而单看场景2，似乎算法B更优。按照普通人的思维，他可能会说，数据少的时候算法A好，数据多的时候算法B好，然后还津津乐道自己懂得辩证法，懂得具体问题具体分析。但计算机科学比较认死理，不搞变通，不玩辩证法，它要求我们制定一个明确的、一致的标准，不要一会儿这样，一会儿那样。那么问题就来了，我们应该怎样制定这个标准呢？


高德纳的思想主要包括以下几个部分。

1．**在比较算法的快慢时，只需要考虑数据量特别大，大到近乎无穷大时的情况**。为什么要比大数的情况，而不比小数的情况呢？因为计算机的发明就是为了处理大量数据的，而且数据越处理越多。比如我和同学们做砸的那个对账功能，就是没有考虑数据量会剧增。

2．**决定算法快慢的因素虽然可能有很多，但是所有的因素都可以被分为两类：第一类是不随数据量变化的因素，第二类是随数据量变化的因素**。

比如说有两种算法：第一种的运算次数是$3N^2$，其中N是处理的数据量；第二种则是100NlogN。前面的那个3也好，这里的100也罢，都是常数，和N的大小显然没有关系，处理10个、100个、1亿个数，都是如此。但是后面和N有关的部分则不同，当N很大时，N2要比NlogN大得多。在处理几千，甚至几万个数字的时候，这两种算法差异不明显，但是高德纳认为，我们衡量算法好坏时，只需要考虑N近乎无穷大的情况。为什么这么考虑问题呢？因为计算机的任务是处理远远超出我们想象的规模的数据量，而我们的认知其实很难想象那样规模的数据有多少。

### 例题1.1

围棋有多复杂？

答：棋盘上每一个点最终可以是黑子、白子或者空位三种情况，而棋盘有361个交叉点，因此围棋的变化最多可以有3361≈2×$10^{172}$种情况。这个数当然相当大，大约是2后面跟172个零。

### 例题1.2

一句有20个单词的英语语句可以有多少种组合？

答：英语的单词数在10万个以上，这里就算是10万个（即$10^5$），20个单词不受限制的组合数是$((10)^5)^{20}=10^{100}$。

